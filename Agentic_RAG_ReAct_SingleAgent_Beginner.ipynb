{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1XOcDYIDPK_"
      },
      "source": [
        "# Agentic RAG (Single Agent) — ReAct + Tavily + Memory + Planning (Beginner Notebook)\n",
        "\n",
        "This notebook demonstrates a **beginner-friendly Agentic RAG system** using a **single ReAct agent**.\n",
        "\n",
        "You will see:\n",
        "- **ReAct framework**: the agent alternates between *reasoning* and *tool use* (search, memory).\n",
        "- **Planning**: the system creates a plan before running the agent.\n",
        "- **Self-reflection**: the system checks and improves the answer after the first draft.\n",
        "- **Managing tool inventory**: tools are registered in one place and passed to the agent.\n",
        "- **Managing memory**:\n",
        "  - Short-term: chat history (ConversationBufferMemory)\n",
        "  - Long-term: a tiny “Notes Memory” tool the agent can write/read\n",
        "\n",
        "> ⚠️ Disclaimer: This is for learning and audit-style research support only — **NOT legal/tax advice**.\n"
      ],
      "id": "k1XOcDYIDPK_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQzSZ0yZDPLB"
      },
      "source": [
        "## What is “Agentic RAG”?\n",
        "\n",
        "Classic RAG is usually: **retrieve → generate** (fixed pipeline).\n",
        "\n",
        "**Agentic RAG** is: **agent decides** when to retrieve, what to retrieve, how many times, and how to combine evidence — using tools.\n"
      ],
      "id": "PQzSZ0yZDPLB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1LXd5FYDPLC"
      },
      "source": [
        "## 0) Install dependencies (single cell)\n",
        "\n",
        "If you re-run the notebook, you can re-run this cell safely.\n"
      ],
      "id": "E1LXd5FYDPLC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqB20zFADPLC",
        "outputId": "98469150-d42f-4404-93af-4bc4231fd234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/105.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.8/105.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install -U tavily-python gradio langchain langchain-tavily langchain-openai langchain-community"
      ],
      "id": "qqB20zFADPLC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ4vMd4LDPLD"
      },
      "source": [
        "## 1) Set API keys (environment variables only)\n",
        "\n",
        "We use:\n",
        "- `OPENAI_API_KEY`\n",
        "- `OPENAI_BASE_URL` (optional; default OpenAI endpoint)\n",
        "- `TAVILY_API_KEY`\n"
      ],
      "id": "uZ4vMd4LDPLD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5r8NEgsDPLD"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cell 2 — Configure API keys (safe prompting)\n",
        "import os, getpass\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "OPENAI_BASE_URL = \"https://aibe.mygreatlearning.com/openai/v1\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')"
      ],
      "id": "B5r8NEgsDPLD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X62YAV8DPLD"
      },
      "source": [
        "## 2) Tools: Tavily Web Search + Memory Tools\n",
        "\n",
        "We will build a **small tool inventory**:\n",
        "1) `tavily_search` — web search (snippets + URLs, no scraping)\n",
        "2) `write_note` — save a fact to long-term notes\n",
        "3) `read_notes` — retrieve saved notes\n",
        "4) `clear_notes` — clear notes for a new session\n",
        "\n",
        "This shows how to manage a tool inventory in one place.\n"
      ],
      "id": "7X62YAV8DPLD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpQgYwdZDPLD",
        "outputId": "4a25e04c-65a9-496c-acb5-60d10c850930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tools loaded: ['tavily_search_results_json', 'write_note', 'read_notes', 'clear_notes']\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict, List\n",
        "import time\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.tools import Tool\n",
        "\n",
        "# 1) Tavily tool (LangChain community tool)\n",
        "tavily_search_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "# 2) Simple long-term \"Notes Memory\" (beginner-friendly)\n",
        "NOTES_MEMORY: List[str] = []\n",
        "\n",
        "def write_note(note: str) -> str:\n",
        "    \"\"\"Store a short fact/decision for later.\"\"\"\n",
        "    note = (note or \"\").strip()\n",
        "    if not note:\n",
        "        return \"No note provided.\"\n",
        "    NOTES_MEMORY.append(note)\n",
        "    return f\"Saved note #{len(NOTES_MEMORY)}.\"\n",
        "\n",
        "def read_notes(_: str = \"\") -> str:\n",
        "    \"\"\"Read all stored notes.\"\"\"\n",
        "    if not NOTES_MEMORY:\n",
        "        return \"No notes saved yet.\"\n",
        "    return \"\\n\".join([f\"- {n}\" for n in NOTES_MEMORY])\n",
        "\n",
        "def clear_notes(_: str = \"\") -> str:\n",
        "    \"\"\"Clear the notes memory.\"\"\"\n",
        "    NOTES_MEMORY.clear()\n",
        "    return \"Notes cleared.\"\n",
        "\n",
        "notes_write_tool = Tool(\n",
        "    name=\"write_note\",\n",
        "    func=write_note,\n",
        "    description=\"Write a short fact/decision to long-term notes memory. Input: a single note string.\"\n",
        ")\n",
        "\n",
        "notes_read_tool = Tool(\n",
        "    name=\"read_notes\",\n",
        "    func=read_notes,\n",
        "    description=\"Read the long-term notes memory. Input can be empty.\"\n",
        ")\n",
        "\n",
        "notes_clear_tool = Tool(\n",
        "    name=\"clear_notes\",\n",
        "    func=clear_notes,\n",
        "    description=\"Clear all long-term notes. Input can be empty.\"\n",
        ")\n",
        "\n",
        "TOOLS = [tavily_search_tool, notes_write_tool, notes_read_tool, notes_clear_tool]\n",
        "\n",
        "print(\"✅ Tools loaded:\", [t.name for t in TOOLS])\n"
      ],
      "id": "MpQgYwdZDPLD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0JRfhHYDPLE"
      },
      "source": [
        "## 3) LLM + Short-term memory (chat history)\n",
        "\n",
        "We use:\n",
        "- `ChatOpenAI` as the model\n",
        "- `ConversationBufferMemory` for short-term memory (what the user and agent said)\n",
        "\n",
        "In beginner demos, buffer memory is easiest to understand.\n"
      ],
      "id": "g0JRfhHYDPLE"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y pydantic pydantic-core langchain langchain-core langchain-community langchain-openai\n",
        "!pip -q install -U \\\n",
        "  \"pydantic>=2.7,<3\" \"pydantic-core>=2.18,<3\" \\\n",
        "  \"langchain==0.2.16\" \"langchain-core==0.2.38\" \"langchain-community==0.2.16\" \"langchain-openai==0.1.23\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o8cLWXDDtyy",
        "outputId": "fec820bc-a04d-4881-c75d-9972ab6ac9a6"
      },
      "id": "3o8cLWXDDtyy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.6/463.6 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-tavily 0.2.16 requires langchain<2.0.0,>=0.3.20, but you have langchain 0.2.16 which is incompatible.\n",
            "langchain-tavily 0.2.16 requires langchain-core<2.0.0,>=0.3.15, but you have langchain-core 0.2.38 which is incompatible.\n",
            "langchain-classic 1.0.1 requires langchain-core<2.0.0,>=1.2.5, but you have langchain-core 0.2.38 which is incompatible.\n",
            "langchain-classic 1.0.1 requires langchain-text-splitters<2.0.0,>=1.1.0, but you have langchain-text-splitters 0.2.4 which is incompatible.\n",
            "google-adk 1.21.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.2.38 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-classic\n",
        "! pip install \"numpy==1.26.4\"\n",
        "!pip install -q langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETrqr4ScEv_U",
        "outputId": "c89e2ff3-a676-4191-8065-a8ddaecdeff4"
      },
      "id": "ETrqr4ScEv_U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "kcJ-M79TDPLE",
        "outputId": "d3593cb0-b24c-4988-cf55-be78c9083556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_core.memory'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-572428672.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#from langchain_classic.memory import ConversationBufferMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#from langchain_community.memory import ConversationBufferMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationBufferMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m llm = ChatOpenAI(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/memory/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_importer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m from langchain.memory.buffer import (\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mConversationBufferMemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mConversationStringBufferMemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/memory/buffer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpre_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseChatMemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_prompt_input_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/memory/chat_memory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mInMemoryChatMessageHistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAIMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHumanMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.memory'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "#from langchain_classic.memory import ConversationBufferMemory\n",
        "#from langchain_community.memory import ConversationBufferMemory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        "    base_url=OPENAI_BASE_URL,\n",
        "    timeout=60,\n",
        "    max_retries=2)\n",
        "\n",
        "short_term_memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "print(\"✅ LLM + short-term memory ready.\")"
      ],
      "id": "kcJ-M79TDPLE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f0jOC6CDPLF"
      },
      "source": [
        "## 4) Planning step (before the agent runs)\n",
        "\n",
        "To make agent behavior **predictable for beginners**, we do a small planning step:\n",
        "- Restate the problem\n",
        "- Decide 2–4 search queries\n",
        "- Decide what to store in memory\n",
        "\n",
        "Then we pass that plan into the agent prompt.\n",
        "\n",
        "This is *planning in a single-agent system* (still 1 agent, but structured).\n"
      ],
      "id": "_f0jOC6CDPLF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rm2HkaeDPLF"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def make_plan(question: str, jurisdiction: str = \"General\") -> Dict:\n",
        "    \"\"\"Simple planner: generate search queries + what to remember.\"\"\"\n",
        "    prompt = f\"\"\"You are a planner for an Agentic RAG system.\n",
        "Return STRICT JSON with keys:\n",
        "- problem_restate: string\n",
        "- search_queries: list of 2 to 4 short queries\n",
        "- memory_notes_to_save: list of 1 to 3 notes to save after we find evidence\n",
        "Jurisdiction: {jurisdiction}\n",
        "User question: {question}\n",
        "\"\"\"\n",
        "    resp = llm.invoke(prompt)\n",
        "    # Try parse; if parsing fails, fall back safely.\n",
        "    try:\n",
        "        return json.loads(resp.content)\n",
        "    except Exception:\n",
        "        return {\n",
        "            \"problem_restate\": question,\n",
        "            \"search_queries\": [f\"{jurisdiction} {question}\"],\n",
        "            \"memory_notes_to_save\": [\"Remember to cite official sources and state limits.\"]\n",
        "        }\n",
        "\n",
        "plan = make_plan(\"UAE VAT late registration penalties and key obligations\", jurisdiction=\"UAE\")\n",
        "plan\n"
      ],
      "id": "9rm2HkaeDPLF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99QG1pmgDPLF"
      },
      "source": [
        "## 5) ReAct Agent (single agent)\n",
        "\n",
        "We use LangChain’s ReAct agent pattern:\n",
        "- The prompt encourages the agent to use tools when needed (search, memory).\n",
        "- The AgentExecutor runs the loop until it finishes.\n"
      ],
      "id": "99QG1pmgDPLF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4X3J5yoDPLF"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "SYSTEM_GUARDRAILS = \"\"\"\n",
        "You are an audit-style research assistant.\n",
        "Non-negotiable guardrails:\n",
        "- NOT legal/tax advice.\n",
        "- Do NOT provide filing instructions or tax planning/avoidance.\n",
        "- Use web search snippets only; if information is missing, say so.\n",
        "- Cite sources with [1], [2] ... and include a citations list at the end.\n",
        "- If authoritative sources are insufficient, explicitly say \"Insufficient authoritative guidance found.\"\n",
        "\"\"\".strip()\n",
        "\n",
        "# ReAct prompt template\n",
        "# Note: ReAct agents typically include tool descriptions automatically via create_react_agent.\n",
        "react_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", SYSTEM_GUARDRAILS),\n",
        "    (\"system\", \"Here is the PLAN (follow it, but you may adjust if evidence suggests):\n",
        "{plan_json}\"),\n",
        "    (\"system\", \"Short-term chat history:\n",
        "{chat_history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "agent = create_react_agent(llm=llm, tools=TOOLS, prompt=react_prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=TOOLS,\n",
        "    memory=short_term_memory,\n",
        "    verbose=True,  # set False if you don't want to see the ReAct loop\n",
        "    handle_parsing_errors=True,\n",
        "    max_iterations=8\n",
        ")\n",
        "\n",
        "print(\"✅ ReAct agent ready.\")\n"
      ],
      "id": "j4X3J5yoDPLF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5miKGTlDPLF"
      },
      "source": [
        "## 6) Run the Agentic RAG query\n",
        "\n",
        "Tip for teaching:\n",
        "- Start with a question that clearly needs web search.\n",
        "- Watch the agent decide to call `tavily_search`.\n",
        "- Then watch it write notes via `write_note`.\n"
      ],
      "id": "C5miKGTlDPLF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJmaPM0sDPLF"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional: reset long-term notes before demo\n",
        "clear_notes()\n",
        "\n",
        "question = \"In UAE, what penalties are mentioned for late VAT registration and what are common obligations an auditor checks?\"\n",
        "inputs = {\n",
        "    \"input\": question,\n",
        "    \"plan_json\": json.dumps(make_plan(question, jurisdiction=\"UAE\"), ensure_ascii=False)\n",
        "}\n",
        "result = agent_executor.invoke(inputs)\n",
        "result[\"output\"][:1200]\n"
      ],
      "id": "fJmaPM0sDPLF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT_HHRTuDPLG"
      },
      "source": [
        "## 7) Self-reflection (quality check + revision)\n",
        "\n",
        "A very simple “self-reflecting mechanism”:\n",
        "1) Ask the model to critique the draft against guardrails\n",
        "2) If issues are found, ask it to produce a revised answer\n",
        "\n",
        "This is a common beginner pattern to show self-correction.\n"
      ],
      "id": "CT_HHRTuDPLG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2gz9x1wDPLG"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def reflect_and_improve(question: str, draft: str) -> str:\n",
        "    critique_prompt = f\"\"\"You are a reviewer for an audit-style assistant.\n",
        "Check the DRAFT for:\n",
        "- missing disclaimer\n",
        "- missing citations list\n",
        "- claims not supported by snippets\n",
        "- tax planning/avoidance or filing steps (must be absent)\n",
        "- missing explicit limitations\n",
        "\n",
        "Return:\n",
        "1) A short critique (bullets)\n",
        "2) A revised answer that fixes issues (keep it concise)\n",
        "Question: {question}\n",
        "DRAFT:\n",
        "{draft}\n",
        "\"\"\"\n",
        "    resp = llm.invoke(critique_prompt)\n",
        "    return resp.content\n",
        "\n",
        "draft = result[\"output\"]\n",
        "review = reflect_and_improve(question, draft)\n",
        "review[:1600]\n"
      ],
      "id": "q2gz9x1wDPLG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt4OpknIDPLG"
      },
      "source": [
        "## 8) Managing memory (demo)\n",
        "\n",
        "We show both:\n",
        "- **Short-term memory**: the conversation buffer\n",
        "- **Long-term notes**: what the agent wrote via `write_note`\n",
        "\n",
        "This helps beginners see that memory can be *multiple layers*.\n"
      ],
      "id": "Dt4OpknIDPLG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcR7cKaxDPLG"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"=== Long-term Notes Memory ===\")\n",
        "print(read_notes())\n",
        "\n",
        "print(\"\\n=== Short-term Chat History (last few messages) ===\")\n",
        "hist = short_term_memory.load_memory_variables({})[\"chat_history\"]\n",
        "for msg in hist[-4:]:\n",
        "    print(f\"{msg.type.upper()}: {msg.content[:200]}\")\n"
      ],
      "id": "WcR7cKaxDPLG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htAf1qhqDPLG"
      },
      "source": [
        "## 9) Managing large tool inventories (beginner view)\n",
        "\n",
        "In real systems, you can have dozens of tools. Beginners get overwhelmed.\n",
        "A simple strategy:\n",
        "- Maintain a **Tool Registry** (one list)\n",
        "- Group tools by category\n",
        "- Keep “safe defaults” (search + memory + calculator)\n",
        "\n",
        "Below we print the tool registry and show short descriptions.\n"
      ],
      "id": "htAf1qhqDPLG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YSG-MrJDPLG"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"=== Tool Registry ===\")\n",
        "for t in TOOLS:\n",
        "    # Some tools have different fields; keep printing robust\n",
        "    desc = getattr(t, \"description\", \"\") or getattr(t, \"args\", \"\")\n",
        "    print(f\"- {t.name}: {desc}\")\n"
      ],
      "id": "2YSG-MrJDPLG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ZlARGUDPLH"
      },
      "source": [
        "# End Summary (inside the notebook)\n",
        "\n",
        "### What you built\n",
        "A **single-agent Agentic RAG system** that:\n",
        "- Plans first (simple JSON plan)\n",
        "- Uses **ReAct** to decide when to call tools\n",
        "- Retrieves evidence via **LangChain Tavily** (no scraping)\n",
        "- Uses **memory**:\n",
        "  - short-term chat history\n",
        "  - long-term notes memory tools\n",
        "- Uses **self-reflection** to critique and improve the answer\n",
        "- Handles a **tool inventory** via a registry\n",
        "\n",
        "### Why it matters\n",
        "This pattern scales to real audit/compliance assistants:\n",
        "- Auditors ask scenarios → agent searches → extracts evidence → produces structured notes\n",
        "- Guardrails reduce hallucinations and unsafe advice\n",
        "- Memory helps carry context across multi-turn investigations\n"
      ],
      "id": "o-ZlARGUDPLH"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}