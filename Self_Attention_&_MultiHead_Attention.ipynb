{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer"
      ],
      "metadata": {
        "id": "b6osmsc72PB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(Layer):\n",
        "  def init(self):\n",
        "    super(ScaledDotProductAttention, self).init()\n",
        "\n",
        "  def call(self, query, key, value):\n",
        "    scores = tf.matmul(query, key, transpose_b=True) # shape (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    d_k = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    scaled_scores = scores / tf.sqrt(d_k)\n",
        "    attention_weights = tf.nn.softmax(scaled_scores, axis=-1) # shape (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, value) # shape (batch_size, num_heads, seq_len_q, depth)\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "ezVL67742UZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(Layer):\n",
        " def __init__(self, num_heads, d_model):\n",
        "     super(MultiHeadAttention, self).__init__()\n",
        "     self.num_heads = num_heads\n",
        "     self.d_model = d_model\n",
        "     self.depth = d_model // num_heads\n",
        "\n",
        "     self.wq = tf.keras.layers.Dense(d_model)  # Query transformation\n",
        "     self.wk = tf.keras.layers.Dense(d_model)  # Key transformation\n",
        "     self.wv = tf.keras.layers.Dense(d_model)  # Value transformation\n",
        "     self.dense = tf.keras.layers.Dense(d_model)  # Linear transformation of the output\n",
        "\n",
        " def split_heads(self, x):\n",
        "     batch_size = tf.shape(x)[0]\n",
        "     x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  # (batch_size, seq_len, num_heads, depth)\n",
        "     return tf.transpose(x, perm=(0, 2, 1, 3))  # (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        " def call(self, query, key, value):\n",
        "     query = self.wq(query)  # shape (batch_size, seq_len_q, d_model)\n",
        "     key = self.wk(key)      # shape (batch_size, seq_len_k, d_model)\n",
        "     value = self.wv(value)  # shape (batch_size, seq_len_v, d_model)\n",
        "\n",
        "     query = self.split_heads(query)  # shape (batch_size, num_heads, seq_len_q, depth)\n",
        "     key = self.split_heads(key)      # shape (batch_size, num_heads, seq_len_k, depth)\n",
        "     value = self.split_heads(value)  # shape (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "     output, attention_weights = ScaledDotProductAttention()(query, key, value)  # shape (batch_size, num_heads, seq_len_q, depth)\n",
        "\n",
        "     output = tf.transpose(output, perm=(0, 2, 1, 3))  # shape (batch_size, seq_len_q, num_heads, depth)\n",
        "     output = tf.reshape(output, (tf.shape(output)[0], -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "     return self.dense(output)"
      ],
      "metadata": {
        "id": "elkJVVMm2jiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbRwHBxf2FDR",
        "outputId": "62d10a25-dbf7-4340-e4d6-771422a5b122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10, 64)\n"
          ]
        }
      ],
      "source": [
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog barked at the cat.\",\n",
        "    \"The mouse ran away.\"\n",
        "]\n",
        "\n",
        "# Sample Input: Tokenize and create embedding vector for demonstration\n",
        "# For simplicity, let's use random embeddings here in practice, you would use word embeddings or TF-IDF.\n",
        "embeddings = np.random.rand(len(sentences), 10, 64)  # (batch_size, seq_len, d_model)\n",
        "query = tf.convert_to_tensor(embeddings.astype(np.float32))\n",
        "key = tf.convert_to_tensor(embeddings.astype(np.float32))\n",
        "value = tf.convert_to_tensor(embeddings.astype(np.float32))\n",
        "\n",
        "# Initialize Multi-Head Attention\n",
        "multi_head_attention = MultiHeadAttention(num_heads=4, d_model=64)\n",
        "\n",
        "# Get the attention output\n",
        "attention_output = multi_head_attention(query, key, value)\n",
        "\n",
        "# Print the output shape\n",
        "print(attention_output.shape)  # Expected output: (batch_size, seq_len_q, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "Scaled Dot-Product Attention: The core attention mechanism calculates the attention score by taking the dot product of the query and key tensors. The scores are then scaled (by the square root of the dimension of the keys) and passed through a softmax function to produce attention weights. Finally, these weights are used to compute the weighted sum of the values.\n",
        "\n",
        "Multi-Head Attention: The model is initialized with the number of attention heads and the output dimension.\n",
        "For each input (query, key, value), dense layers are used to project them into the model space.\n",
        "The split_heads function reshapes the projected tensors to separate the different attention heads for parallel processing.\n",
        "The output of the scaled dot-product attention is reshaped and passed through a final dense layer.\n",
        "\n",
        "Example Sentences: The example sentences are tokenized and converted into random embedding vectors (for demonstration). In a real scenario, you would use pre-trained embeddings or trained word vectors.\n",
        "Model Execution: By calling the multi_head_attention layer, you can see the resulting shape of the output tensor."
      ],
      "metadata": {
        "id": "rU_b8-Dd2zHR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DsKecip32n5I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}