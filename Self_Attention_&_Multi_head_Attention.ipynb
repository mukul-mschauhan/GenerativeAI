{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOusiFFCMv0gWDjHCKQAk69",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukul-mschauhan/GenerativeAI/blob/main/Self_Attention_%26_Multi_head_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uABSbXsWoueb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1a54df"
      },
      "source": [
        "# Task\n",
        "Code the attention mechanism from scratch (Self Attention and Multi Head Attention). Take one example sentence and show the calculation of Self and Multi Head Attention. Summarize the whole exercise by suggesting what will happen to these context embeddings. Keep the code simple for beginners and include comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50937039"
      },
      "source": [
        "## Implement self-attention\n",
        "\n",
        "### Subtask:\n",
        "Write Python code to implement the self-attention mechanism from scratch, including the calculation of Query, Key, and Value matrices, the dot product, scaling, softmax, and the final output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a534a886"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to implement the self-attention mechanism as a Python function, including the definition of weight matrices, calculation of Q, K, V, dot product, scaling, softmax, and the final output, based on the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "35eef4ee",
        "outputId": "80f831f4-4ce5-438f-b9c8-56362e5129ae"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def self_attention(input_embeddings):\n",
        "    \"\"\"\n",
        "    Implements the self-attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        input_embeddings: Input embeddings of shape (sequence_length, embedding_dim).\n",
        "\n",
        "    Returns:\n",
        "        Output of the self-attention mechanism of shape (sequence_length, embedding_dim).\n",
        "    \"\"\"\n",
        "    sequence_length, embedding_dim = input_embeddings.shape\n",
        "\n",
        "    # 2. Define learnable weight matrices for Query, Key, and Value (W_Q, W_K, W_V).\n",
        "    #    Initialize these randomly. For simplicity, we assume the output dimension\n",
        "    #    is the same as the input embedding dimension.\n",
        "    np.random.seed(42) # for reproducibility\n",
        "    w_q = np.random.rand(embedding_dim, embedding_dim)\n",
        "    w_k = np.random.rand(embedding_dim, embedding_dim)\n",
        "    w_v = np.random.rand(embedding_dim, embedding_dim)\n",
        "\n",
        "    # 3. Calculate the Query, Key, and Value matrices\n",
        "    query = np.dot(input_embeddings, w_q)\n",
        "    key = np.dot(input_embeddings, w_k)\n",
        "    value = np.dot(input_embeddings, w_v)\n",
        "\n",
        "    # 4. Compute the dot product of the Query and Key matrices.\n",
        "    dot_product = np.dot(query, key.T) # Transpose Key for matrix multiplication\n",
        "\n",
        "    # 5. Scale the dot product\n",
        "    scaling_factor = np.sqrt(embedding_dim)\n",
        "    scaled_dot_product = dot_product / scaling_factor\n",
        "\n",
        "    # 6. Apply the softmax function to the scaled dot product to obtain attention weights.\n",
        "    #    We need to apply softmax row-wise.\n",
        "    #    Subtracting the max for numerical stability\n",
        "    exp_scaled_dot_product = np.exp(scaled_dot_product - np.max(scaled_dot_product, axis=1, keepdims=True))\n",
        "    attention_weights = exp_scaled_dot_product / np.sum(exp_scaled_dot_product, axis=1, keepdims=True)\n",
        "\n",
        "    # 7. Multiply the attention weights by the Value matrix to get the final self-attention output.\n",
        "    output = np.dot(attention_weights, value)\n",
        "\n",
        "    # 8. Return the self-attention output.\n",
        "    return output\n",
        "\n",
        "# Example Usage:\n",
        "# Let's create a dummy input embedding for a sentence with 3 words and embedding dimension 4.\n",
        "dummy_input_embeddings = np.array([\n",
        "    [1.0, 0.5, 0.1, 0.9],  # Embedding for word 1\n",
        "    [0.2, 0.8, 0.3, 0.7],  # Embedding for word 2\n",
        "    [0.6, 0.4, 0.9, 0.2]   # Embedding for word 3\n",
        "])\n",
        "\n",
        "attention_output = self_attention(dummy_input_embeddings)\n",
        "print(\"Input Embeddings Shape:\", dummy_input_embeddings.shape)\n",
        "print(\"Self-Attention Output Shape:\", attention_output.shape)\n",
        "print(\"\\nSelf-Attention Output:\")\n",
        "display(attention_output)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Embeddings Shape: (3, 4)\n",
            "Self-Attention Output Shape: (3, 4)\n",
            "\n",
            "Self-Attention Output:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0.41424831, 1.28155963, 1.19660905, 1.47464873],\n",
              "       [0.41537054, 1.2600382 , 1.18050333, 1.46096009],\n",
              "       [0.41012037, 1.269359  , 1.1786554 , 1.47522522]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c454d4f"
      },
      "source": [
        "## Implement multi-head attention\n",
        "\n",
        "### Subtask:\n",
        "Write Python code to implement the multi-head attention mechanism from scratch, building upon the self-attention implementation. This will involve splitting the input into multiple heads, applying self-attention to each head, and concatenating the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dabc2309"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `multi_head_attention` function, calculate head dimensions, and implement the splitting and linear transformations for Q, K, V, and the final output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "4e336091",
        "outputId": "a8188b52-4945-41a6-94f4-dad384b05aa2"
      },
      "source": [
        "def multi_head_attention(input_embeddings, num_heads):\n",
        "    \"\"\"\n",
        "    Implements the multi-head attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        input_embeddings: Input embeddings of shape (sequence_length, embedding_dim).\n",
        "        num_heads: The number of attention heads.\n",
        "\n",
        "    Returns:\n",
        "        Output of the multi-head attention mechanism of shape (sequence_length, embedding_dim).\n",
        "    \"\"\"\n",
        "    sequence_length, embedding_dim = input_embeddings.shape\n",
        "\n",
        "    # 2. Calculate the dimension of each head.\n",
        "    if embedding_dim % num_heads != 0:\n",
        "        raise ValueError(\"Embedding dimension must be divisible by the number of heads\")\n",
        "    head_dim = embedding_dim // num_heads\n",
        "\n",
        "    # 4. Initialize weight matrices for Query, Key, Value, and the final output.\n",
        "    #    For simplicity, we'll initialize single larger matrices and reshape/split them later.\n",
        "    np.random.seed(42) # for reproducibility\n",
        "    w_q_all = np.random.rand(embedding_dim, embedding_dim)\n",
        "    w_k_all = np.random.rand(embedding_dim, embedding_dim)\n",
        "    w_v_all = np.random.rand(embedding_dim, embedding_dim)\n",
        "    w_o = np.random.rand(embedding_dim, embedding_dim)\n",
        "\n",
        "    # 3. Split the input embeddings into multiple \"heads\" - this will be done implicitly\n",
        "    #    by splitting the Q, K, V matrices after the initial linear transformation.\n",
        "\n",
        "    # 5. Calculate the Query, Key, and Value matrices for all heads combined initially.\n",
        "    query_all = np.dot(input_embeddings, w_q_all)\n",
        "    key_all = np.dot(input_embeddings, w_k_all)\n",
        "    value_all = np.dot(input_embeddings, w_v_all)\n",
        "\n",
        "    # Reshape Q, K, V to split into multiple heads\n",
        "    # Shape becomes (sequence_length, num_heads, head_dim)\n",
        "    query_all_reshaped = query_all.reshape(sequence_length, num_heads, head_dim)\n",
        "    key_all_reshaped = key_all.reshape(sequence_length, num_heads, head_dim)\n",
        "    value_all_reshaped = value_all.reshape(sequence_length, num_heads, head_dim)\n",
        "\n",
        "    # Transpose to (num_heads, sequence_length, head_dim) for easier processing per head\n",
        "    query_all_transposed = query_all_reshaped.transpose(1, 0, 2)\n",
        "    key_all_transposed = key_all_reshaped.transpose(1, 0, 2)\n",
        "    value_all_transposed = value_all_reshaped.transpose(1, 0, 2)\n",
        "\n",
        "\n",
        "    attention_outputs_per_head = []\n",
        "    # 6. Apply self_attention to each head.\n",
        "    for i in range(num_heads):\n",
        "        q_head = query_all_transposed[i] # Shape (sequence_length, head_dim)\n",
        "        k_head = key_all_transposed[i]   # Shape (sequence_length, head_dim)\n",
        "        v_head = value_all_transposed[i] # Shape (sequence_length, head_dim)\n",
        "\n",
        "        # Apply self-attention logic (dot product, scaling, softmax, multiply by V)\n",
        "        # This part mirrors the self_attention function logic, but applied to individual heads\n",
        "        dot_product_head = np.dot(q_head, k_head.T)\n",
        "        scaling_factor_head = np.sqrt(head_dim)\n",
        "        scaled_dot_product_head = dot_product_head / scaling_factor_head\n",
        "\n",
        "        exp_scaled_dot_product_head = np.exp(scaled_dot_product_head - np.max(scaled_dot_product_head, axis=1, keepdims=True))\n",
        "        attention_weights_head = exp_scaled_dot_product_head / np.sum(exp_scaled_dot_product_head, axis=1, keepdims=True)\n",
        "\n",
        "        output_head = np.dot(attention_weights_head, v_head) # Shape (sequence_length, head_dim)\n",
        "        attention_outputs_per_head.append(output_head)\n",
        "\n",
        "    # 7. Concatenate the outputs of all the heads.\n",
        "    # Shape becomes (num_heads, sequence_length, head_dim)\n",
        "    concatenated_heads = np.stack(attention_outputs_per_head, axis=0)\n",
        "    # Reshape to (sequence_length, num_heads * head_dim), which is (sequence_length, embedding_dim)\n",
        "    concatenated_heads_reshaped = concatenated_heads.transpose(1, 0, 2).reshape(sequence_length, embedding_dim)\n",
        "\n",
        "    # 8. Apply a final linear transformation.\n",
        "    final_output = np.dot(concatenated_heads_reshaped, w_o)\n",
        "\n",
        "    # 9. Return the final multi-head attention output.\n",
        "    return final_output\n",
        "\n",
        "# Example Usage:\n",
        "# Let's use the same dummy input embedding and test with 2 heads.\n",
        "dummy_input_embeddings = np.array([\n",
        "    [1.0, 0.5, 0.1, 0.9],  # Embedding for word 1\n",
        "    [0.2, 0.8, 0.3, 0.7],  # Embedding for word 2\n",
        "    [0.6, 0.4, 0.9, 0.2]   # Embedding for word 3\n",
        "])\n",
        "num_heads = 2 # Must be a divisor of embedding_dim (4)\n",
        "\n",
        "multi_head_output = multi_head_attention(dummy_input_embeddings, num_heads)\n",
        "\n",
        "print(\"Input Embeddings Shape:\", dummy_input_embeddings.shape)\n",
        "print(\"Multi-Head Attention Output Shape:\", multi_head_output.shape)\n",
        "print(\"\\nMulti-Head Attention Output:\")\n",
        "display(multi_head_output)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Embeddings Shape: (3, 4)\n",
            "Multi-Head Attention Output Shape: (3, 4)\n",
            "\n",
            "Multi-Head Attention Output:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[2.08600928, 1.83908121, 2.41701368, 2.39544226],\n",
              "       [2.07620086, 1.8254594 , 2.41172336, 2.3813065 ],\n",
              "       [2.08055114, 1.832296  , 2.41240516, 2.38511854]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc79083a"
      },
      "source": [
        "## Example calculation\n",
        "\n",
        "### Subtask:\n",
        "Choose a simple sentence and demonstrate the step-by-step calculation of self-attention and multi-head attention using the implemented code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4da1c6ee"
      },
      "source": [
        "**Reasoning**:\n",
        "Choose a simple sentence, create dummy input embeddings, and then call the self-attention and multi-head attention functions with these inputs to demonstrate the calculations and print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "150feb7d",
        "outputId": "c536d8ac-43d8-499d-8644-f139174ecad8"
      },
      "source": [
        "# 1. Choose a simple sentence\n",
        "sentence = \"Hello world\"\n",
        "words = sentence.split()\n",
        "sequence_length = len(words)\n",
        "embedding_dim = 4 # Choose a simple embedding dimension\n",
        "\n",
        "print(f\"Chosen sentence: '{sentence}'\")\n",
        "print(f\"Words: {words}\")\n",
        "print(f\"Sequence length: {sequence_length}\")\n",
        "print(f\"Embedding dimension: {embedding_dim}\")\n",
        "\n",
        "# 2. Create dummy input embeddings\n",
        "# Shape: (sequence_length, embedding_dim)\n",
        "np.random.seed(0) # Set seed for reproducibility\n",
        "input_embeddings = np.random.rand(sequence_length, embedding_dim)\n",
        "\n",
        "# 3. Print the chosen sentence and the dummy input embeddings\n",
        "print(\"\\nDummy Input Embeddings:\")\n",
        "display(input_embeddings)\n",
        "\n",
        "# 4. Call the self_attention function and print the output\n",
        "print(\"\\nCalculating Self-Attention:\")\n",
        "self_attention_output = self_attention(input_embeddings)\n",
        "print(\"Self-Attention Output:\")\n",
        "display(self_attention_output)\n",
        "\n",
        "# 5. Call the multi_head_attention function and print the output\n",
        "print(f\"\\nCalculating Multi-Head Attention with {num_heads} heads:\")\n",
        "# Ensure embedding_dim is divisible by num_heads\n",
        "if embedding_dim % num_heads != 0:\n",
        "    raise ValueError(f\"Embedding dimension ({embedding_dim}) must be divisible by the number of heads ({num_heads})\")\n",
        "\n",
        "multi_head_attention_output = multi_head_attention(input_embeddings, num_heads)\n",
        "print(\"Multi-Head Attention Output:\")\n",
        "display(multi_head_attention_output)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen sentence: 'Hello world'\n",
            "Words: ['Hello', 'world']\n",
            "Sequence length: 2\n",
            "Embedding dimension: 4\n",
            "\n",
            "Dummy Input Embeddings:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0.5488135 , 0.71518937, 0.60276338, 0.54488318],\n",
              "       [0.4236548 , 0.64589411, 0.43758721, 0.891773  ]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating Self-Attention:\n",
            "Self-Attention Output:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0.48703578, 1.26063123, 1.17903779, 1.5423837 ],\n",
              "       [0.4873165 , 1.26078774, 1.17857997, 1.54167794]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating Multi-Head Attention with 2 heads:\n",
            "Multi-Head Attention Output:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[2.15381008, 1.8670745 , 2.55620525, 2.47266139],\n",
              "       [2.15397661, 1.86714714, 2.55631435, 2.47285283]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f213d7e4"
      },
      "source": [
        "## Summarize embedding transformation\n",
        "\n",
        "### Subtask:\n",
        "Explain how the attention mechanism transforms the input embeddings into context embeddings, highlighting the role of weighted sums and the capture of dependencies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2901185"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain how the attention mechanism transforms input embeddings into context embeddings, covering the role of input embeddings, attention weights, weighted sums, and how this captures dependencies. Also, briefly mention multi-head attention's role.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1bb37f",
        "outputId": "34b7f854-31d6-49da-e7de-6bd9c14aa633"
      },
      "source": [
        "# 1. Explain how input embeddings represent individual words or tokens.\n",
        "print(\"1. Input Embeddings:\")\n",
        "print(\"   Input embeddings are numerical representations of individual words or tokens.\")\n",
        "print(\"   They are typically dense vectors where similar words have similar vector representations.\")\n",
        "print(\"   These embeddings are the starting point for the attention mechanism.\")\n",
        "print(f\"   In our example, the input embeddings have a shape of (sequence_length={sequence_length}, embedding_dim={embedding_dim}).\")\n",
        "print(f\"   Each row represents a word's initial embedding.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Describe how the attention mechanism calculates attention weights.\n",
        "print(\"2. Calculating Attention Weights:\")\n",
        "print(\"   The core idea of attention is to determine how much each input element should 'attend' to other input elements.\")\n",
        "print(\"   This is done by calculating attention weights based on the relationships between Query (Q), Key (K), and Value (V) vectors.\")\n",
        "print(\"   - Query: Represents the current element's desire to find relevant information in other elements.\")\n",
        "print(\"   - Key: Represents the information that each element makes available to others.\")\n",
        "print(\"   - Value: Represents the actual content or information of each element.\")\n",
        "print(\"   The attention weights are typically calculated by taking the dot product of the Query of one element with the Keys of all other elements, scaling, and applying a softmax function.\")\n",
        "print(\"   The softmax ensures that the weights for a given query sum up to 1, indicating a probability distribution over the input elements.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Explain that the attention output is a weighted sum of the Value vectors.\n",
        "print(\"3. Weighted Sum of Value Vectors:\")\n",
        "print(\"   Once the attention weights are calculated, the output for each input element is computed as a weighted sum of the Value vectors of *all* input elements.\")\n",
        "print(\"   Each Value vector is multiplied by its corresponding attention weight (how much the current element should attend to that specific element's Value).\")\n",
        "print(\"   These weighted Value vectors are then summed up to produce the final output vector for the current input element.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. Discuss how this captures dependencies and creates \"context embeddings\".\n",
        "print(\"4. Capturing Dependencies and Creating Context Embeddings:\")\n",
        "print(\"   This weighted sum is crucial because it allows the model to incorporate information from all other words in the sequence when representing a single word.\")\n",
        "print(\"   Words that are highly relevant to the current word (as determined by the attention weights) contribute more significantly to its output representation.\")\n",
        "print(\"   Crucially, this mechanism can capture dependencies between words regardless of their distance in the sequence, unlike traditional recurrent neural networks (RNNs).\")\n",
        "print(\"   The resulting output vectors are often referred to as 'context embeddings' because they are no longer just representations of individual words in isolation, but are enriched with contextual information from the entire sequence.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 5. Briefly mention multi-head attention.\n",
        "print(\"5. Role of Multi-Head Attention:\")\n",
        "print(\"   Multi-head attention enhances this process by performing the attention calculation multiple times in parallel, using different sets of learned Query, Key, and Value weight matrices for each 'head'.\")\n",
        "print(\"   This allows the model to attend to different aspects of the relationships between words simultaneously.\")\n",
        "print(\"   For example, one head might focus on syntactic relationships, while another might focus on semantic relationships.\")\n",
        "print(\"   The outputs from the different heads are then concatenated and linearly transformed to produce the final multi-head attention output.\")\n",
        "print(f\"   In our example, we used {num_heads} heads, allowing the model to capture {num_heads} different types of relationships concurrently.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nIn summary, the attention mechanism transforms initial word embeddings into context embeddings by dynamically weighting the contributions of all other words in the sequence based on their relevance, thereby capturing complex dependencies.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Input Embeddings:\n",
            "   Input embeddings are numerical representations of individual words or tokens.\n",
            "   They are typically dense vectors where similar words have similar vector representations.\n",
            "   These embeddings are the starting point for the attention mechanism.\n",
            "   In our example, the input embeddings have a shape of (sequence_length=2, embedding_dim=4).\n",
            "   Each row represents a word's initial embedding.\n",
            "--------------------------------------------------\n",
            "2. Calculating Attention Weights:\n",
            "   The core idea of attention is to determine how much each input element should 'attend' to other input elements.\n",
            "   This is done by calculating attention weights based on the relationships between Query (Q), Key (K), and Value (V) vectors.\n",
            "   - Query: Represents the current element's desire to find relevant information in other elements.\n",
            "   - Key: Represents the information that each element makes available to others.\n",
            "   - Value: Represents the actual content or information of each element.\n",
            "   The attention weights are typically calculated by taking the dot product of the Query of one element with the Keys of all other elements, scaling, and applying a softmax function.\n",
            "   The softmax ensures that the weights for a given query sum up to 1, indicating a probability distribution over the input elements.\n",
            "--------------------------------------------------\n",
            "3. Weighted Sum of Value Vectors:\n",
            "   Once the attention weights are calculated, the output for each input element is computed as a weighted sum of the Value vectors of *all* input elements.\n",
            "   Each Value vector is multiplied by its corresponding attention weight (how much the current element should attend to that specific element's Value).\n",
            "   These weighted Value vectors are then summed up to produce the final output vector for the current input element.\n",
            "--------------------------------------------------\n",
            "4. Capturing Dependencies and Creating Context Embeddings:\n",
            "   This weighted sum is crucial because it allows the model to incorporate information from all other words in the sequence when representing a single word.\n",
            "   Words that are highly relevant to the current word (as determined by the attention weights) contribute more significantly to its output representation.\n",
            "   Crucially, this mechanism can capture dependencies between words regardless of their distance in the sequence, unlike traditional recurrent neural networks (RNNs).\n",
            "   The resulting output vectors are often referred to as 'context embeddings' because they are no longer just representations of individual words in isolation, but are enriched with contextual information from the entire sequence.\n",
            "--------------------------------------------------\n",
            "5. Role of Multi-Head Attention:\n",
            "   Multi-head attention enhances this process by performing the attention calculation multiple times in parallel, using different sets of learned Query, Key, and Value weight matrices for each 'head'.\n",
            "   This allows the model to attend to different aspects of the relationships between words simultaneously.\n",
            "   For example, one head might focus on syntactic relationships, while another might focus on semantic relationships.\n",
            "   The outputs from the different heads are then concatenated and linearly transformed to produce the final multi-head attention output.\n",
            "   In our example, we used 2 heads, allowing the model to capture 2 different types of relationships concurrently.\n",
            "--------------------------------------------------\n",
            "\n",
            "In summary, the attention mechanism transforms initial word embeddings into context embeddings by dynamically weighting the contributions of all other words in the sequence based on their relevance, thereby capturing complex dependencies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d107f2d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The self-attention mechanism was successfully implemented in Python, demonstrating the calculation of Query, Key, and Value matrices, scaled dot product, softmax, and weighted sum to produce context embeddings.\n",
        "*   The multi-head attention mechanism was implemented by splitting the input into multiple heads, applying the self-attention logic independently to each head, concatenating the results, and applying a final linear transformation.\n",
        "*   Using the simple sentence \"Hello world\" and dummy input embeddings, both the self-attention and multi-head attention functions were successfully applied, producing output embeddings of the same shape as the input embeddings, indicating the creation of context-aware representations.\n",
        "*   The explanation detailed how the attention mechanism calculates attention weights based on Query, Key, and Value relationships and uses these weights to compute a weighted sum of Value vectors, effectively capturing dependencies between words and creating context embeddings.\n",
        "*   Multi-head attention was explained as a method to capture different aspects of word relationships simultaneously by running the attention process in parallel across multiple heads.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The resulting context embeddings from both self-attention and multi-head attention layers serve as richer representations of words that incorporate information from the entire sequence. These embeddings can then be used as input for subsequent layers in a neural network (e.g., feed-forward networks) for downstream tasks like classification, translation, or text generation.\n",
        "*   Further exploration could involve visualizing the attention weights for the example sentence to understand which words each word is attending to, providing insight into the captured dependencies.\n"
      ]
    }
  ]
}