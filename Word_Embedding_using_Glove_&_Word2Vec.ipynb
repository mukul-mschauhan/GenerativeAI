{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embedding using Glove..."
      ],
      "metadata": {
        "id": "0jgrPas-XMQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gensim\n",
        "#!pip install numpy==1.24.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "w4cLF-yVXSY5",
        "outputId": "5fc612ef-0665-491e-f3db-1c7046bd3560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m477.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "30b54007452943a89a5a5d73e736c93f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embeddings...\n",
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-100\")"
      ],
      "metadata": {
        "id": "65xBKGtun3jP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7eaa2fa-6c30-4dd2-fb41-84b5f9740365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most Similar....\n",
        "model.most_similar([model[\"king\"]], topn = 10)\n",
        "\n",
        "# Cosine Similarity - It will return if two words are similar in nature..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7tFuKUbXOY-",
        "outputId": "076c19fa-5031-4205-a060-b5f5272b11c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 1.0),\n",
              " ('prince', 0.7682328820228577),\n",
              " ('queen', 0.7507690787315369),\n",
              " ('son', 0.7020888328552246),\n",
              " ('brother', 0.6985775232315063),\n",
              " ('monarch', 0.6977890729904175),\n",
              " ('throne', 0.6919989585876465),\n",
              " ('kingdom', 0.6811409592628479),\n",
              " ('father', 0.6802029013633728),\n",
              " ('emperor', 0.6712858080863953)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar([model[\"newyork\"]], topn = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LecNuMZpcow3",
        "outputId": "22efc092-76ed-4e53-e761-3d835743dd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('newyork', 1.0),\n",
              " ('salpetriere', 0.5204809308052063),\n",
              " ('pitie', 0.5070111155509949),\n",
              " ('york-presbyterian', 0.50453782081604),\n",
              " ('maximilians', 0.496698796749115),\n",
              " ('kettering', 0.4923306703567505),\n",
              " ('baystate', 0.4880768656730652),\n",
              " ('meeschaert', 0.48763978481292725),\n",
              " ('shands', 0.4872145354747772),\n",
              " ('deaconess', 0.48343440890312195)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vector for King\n",
        "king = model[\"king\"]\n",
        "\n",
        "# Vector for Queen\n",
        "queen = model[\"queen\"]\n",
        "\n",
        "# Vector for employee\n",
        "employee = model[\"employee\"]\n",
        "\n",
        "print(queen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEAWW93pXy8w",
        "outputId": "385f5086-7a16-4a26-960b-9b019405a565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.50045  -0.70826   0.55388   0.673     0.22486   0.60281  -0.26194\n",
            "  0.73872  -0.65383  -0.21606  -0.33806   0.24498  -0.51497   0.8568\n",
            " -0.37199  -0.58824   0.30637  -0.30668  -0.2187    0.78369  -0.61944\n",
            " -0.54925   0.43067  -0.027348  0.97574   0.46169   0.11486  -0.99842\n",
            "  1.0661   -0.20819   0.53158   0.40922   1.0406    0.24943   0.18709\n",
            "  0.41528  -0.95408   0.36822  -0.37948  -0.6802   -0.14578  -0.20113\n",
            "  0.17113  -0.55705   0.7191    0.070014 -0.23637   0.49534   1.1576\n",
            " -0.05078   0.25731  -0.091052  1.2663    1.1047   -0.51584  -2.0033\n",
            " -0.64821   0.16417   0.32935   0.048484  0.18997   0.66116   0.080882\n",
            "  0.3364    0.22758   0.1462   -0.51005   0.63777   0.47299  -0.3282\n",
            "  0.083899 -0.78547   0.099148  0.039176  0.27893   0.11747   0.57862\n",
            "  0.043639 -0.15965  -0.35304  -0.048965 -0.32461   1.4981    0.58138\n",
            " -1.132    -0.60673  -0.37505  -1.1813    0.80117  -0.50014  -0.16574\n",
            " -0.70584   0.43012   0.51051  -0.8033   -0.66572  -0.63717  -0.36032\n",
            "  0.13347  -0.56075 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for Cosine Similarity\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "boss = model[\"england\"]\n",
        "employee = model[\"new-england\"]\n",
        "\n",
        "# King Vs Queen\n",
        "cos_sim = dot(boss, employee)/(norm(boss)*norm(employee))\n",
        "cos_sim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB0ylb__YiBk",
        "outputId": "b2494d82-005a-4592-99a9-b21df1a96149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.29185092"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec Model from Gensim\n",
        "\n",
        "Generating Embedding from a Sentence.."
      ],
      "metadata": {
        "id": "np10onw1Y_un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "# Step 1: Preprocess the sentence\n",
        "sentence = 'the quick brown fox jumped over the lazy dog'\n",
        "print(\"Original Sentence:\", sentence)\n",
        "\n",
        "# Step 2: Tokenize\n",
        "words = sentence.split()\n",
        "print(\"Tokenized Words:\", words)\n",
        "\n",
        "# Step 3: Wrap in list as Word2Vec expects list of list of tokens\n",
        "corpus = [words]\n",
        "\n",
        "# Step 4: Train Word2Vec Model\n",
        "model = Word2Vec(sentences=corpus, vector_size=10,\n",
        "                 window=2, min_count=1, sg=1)  # sg=1 uses Skip-Gram\n",
        "\n",
        "# Step 5: Create vocab maps\n",
        "vocab = set(words)\n",
        "word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx2word = {i: word for word, i in word2idx.items()}\n",
        "print(\"word2idx:\", word2idx)\n",
        "print(\"idx2word:\", idx2word)\n",
        "\n",
        "# Step 6: Get embedding vector for a word\n",
        "print(\"\\nEmbedding for 'fox':\")\n",
        "print(model.wv['fox'])\n",
        "\n",
        "# Step 7: Find similar words (based on cosine similarity)\n",
        "print(\"\\nWords most similar to 'fox':\")\n",
        "print(model.wv.most_similar('fox'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AxV2s2iYjPm",
        "outputId": "3d294ddf-db61-41cb-b785-8310d25f6e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: the quick brown fox jumped over the lazy dog\n",
            "Tokenized Words: ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
            "word2idx: {'fox': 0, 'brown': 1, 'lazy': 2, 'jumped': 3, 'dog': 4, 'quick': 5, 'the': 6, 'over': 7}\n",
            "idx2word: {0: 'fox', 1: 'brown', 2: 'lazy', 3: 'jumped', 4: 'dog', 5: 'quick', 6: 'the', 7: 'over'}\n",
            "\n",
            "Embedding for 'fox':\n",
            "[-0.08157917  0.04495798 -0.04137076  0.00824536  0.08498619 -0.04462177\n",
            "  0.045175   -0.0678696  -0.03548489  0.09398508]\n",
            "\n",
            "Words most similar to 'fox':\n",
            "[('jumped', 0.24953821301460266), ('over', 0.09267304837703705), ('quick', -0.021806996315717697), ('brown', -0.042645372450351715), ('lazy', -0.15169015526771545), ('the', -0.2726021111011505), ('dog', -0.38205230236053467)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of OOV....\n",
        "print(\"\\nWords most similar to 'New York':\")\n",
        "print(model.wv.most_similar('new york'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "bclV4ZpPZ7JQ",
        "outputId": "48876941-3d6f-433f-f1cc-3f42ac738434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words most similar to 'New York':\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'new york' not present in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-9cc347948bb9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# example of OOV....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nWords most similar to 'New York':\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new york'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'new york' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling OOV using Phrase Detection\n",
        "\n",
        "* Phrase detection helps identify multi-word expressions (like ``new-york`` and ``San Francisco``) and treat them as a single unit.\n",
        "\n",
        "* This way, ``New York`` is included in the vocabulary as ``new_york``, making it no longer OOV.\n",
        "\n",
        "### Why Is This Helpful?\n",
        "* Captures semantic meaning of phrases like ``United Nation``, ``New York``, ``data science`` as a whole.\n",
        "\n",
        "* Improves **Word2Vec accuracy** — these phrases often have a unique meaning that's different from the sum of individual words.\n",
        "\n",
        "* Avoids OOV errors for common named entities and idiomatic expressions.\n"
      ],
      "metadata": {
        "id": "MS_vm474aQtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Required Libraries..\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "from gensim.models.phrases import Phraser"
      ],
      "metadata": {
        "id": "C70ktDMDZ9lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start with Sample Corpus containing Token of Sentences"
      ],
      "metadata": {
        "id": "sUkm08P1ccc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample corpus with phrases\n",
        "sentences = [\n",
        "    ['i', 'live', 'in', 'new', 'york', 'city'],\n",
        "    ['new', 'york', 'is', 'great'],\n",
        "    ['san', 'francisco', 'is', 'beautiful']]"
      ],
      "metadata": {
        "id": "fvi2pyqqaW9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Detect Phrases"
      ],
      "metadata": {
        "id": "E6NNuUEGcVsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Detect Phrases (like 'new_york')\n",
        "bigram = Phrases(sentences, min_count=1, threshold=1)\n",
        "bigram_phraser = Phraser(bigram)\n",
        "\n",
        "# Step 2: Apply phrase detection to the corpus\n",
        "phrased_sentences = list(bigram_phraser[sentences])\n",
        "print(\"After Phrase Detection:\", phrased_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40-o1bTabowV",
        "outputId": "6fae2705-ca05-4042-bb70-c42f8ebd052f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Phrase Detection: [['i', 'live', 'in', 'new_york', 'city'], ['new_york', 'is', 'great'], ['san', 'francisco', 'is', 'beautiful']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train Word2Vec Model and Generate Embeddings"
      ],
      "metadata": {
        "id": "RC1_jjdqcXxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train Word2Vec on updated corpus\n",
        "word2vecmodel = Word2Vec(sentences=phrased_sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
        "\n",
        "# Step 4: Check embedding\n",
        "print(\"\\nEmbedding for 'new_york':\")\n",
        "print(word2vecmodel.wv['new_york'])\n",
        "\n",
        "# Step 5: Similar words\n",
        "print(\"\\nWords most similar to 'new_york':\")\n",
        "print(word2vecmodel.wv.most_similar('new_york'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avIfZibpcSuN",
        "outputId": "b082561e-8c3e-4e55-8c7b-dc77232fd303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Embedding for 'new_york':\n",
            "[-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
            "  0.009035   -0.01357392 -0.00709698  0.01879702 -0.00315531  0.00064274\n",
            " -0.00828126 -0.01536538 -0.00301602  0.00493959 -0.00177605  0.01106732\n",
            " -0.00548595  0.00452013  0.01091159  0.01669191 -0.00290748 -0.01841629\n",
            "  0.0087411   0.00114357  0.01488382 -0.00162657 -0.00527683 -0.01750602\n",
            " -0.00171311  0.00565313  0.01080286  0.01410531 -0.01140624  0.00371764\n",
            "  0.01217773 -0.0095961  -0.00621452  0.01359526  0.00326295  0.00037983\n",
            "  0.00694727  0.00043555  0.01923765  0.01012121 -0.01783478 -0.01408312\n",
            "  0.00180291  0.01278507]\n",
            "\n",
            "Words most similar to 'new_york':\n",
            "[('san', 0.12486357241868973), ('city', 0.07399576157331467), ('is', 0.04237300902605057), ('great', 0.018277151510119438), ('beautiful', 0.011078279465436935), ('i', 0.0013571369927376509), ('live', -0.1191045343875885), ('francisco', -0.17424818873405457), ('in', -0.1754782646894455)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1bkXA1m-d0ot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}